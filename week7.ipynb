{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fb50d30",
   "metadata": {},
   "source": [
    "# Week 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01306356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting surpriseNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Using cached surprise-0.1-py2.py3-none-any.whl (1.8 kB)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.3.3-cp310-cp310-win_amd64.whl (11.3 MB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.2-cp310-cp310-win_amd64.whl (8.9 MB)\n",
      "     ---------------------------------------- 8.9/8.9 MB 5.4 MB/s eta 0:00:00\n",
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.10.8-cp310-cp310-win_amd64.whl (8.1 MB)\n",
      "Collecting seaborn\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Collecting scikit-surprise\n",
      "  Using cached scikit_surprise-1.1.4-cp310-cp310-win_amd64.whl\n",
      "Collecting tzdata>=2022.7\n",
      "  Using cached tzdata-2025.3-py2.py3-none-any.whl (348 kB)\n",
      "Collecting numpy>=1.22.4\n",
      "  Using cached numpy-2.2.6-cp310-cp310-win_amd64.whl (12.9 MB)\n",
      "Collecting pytz>=2020.1\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\armin\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting threadpoolctl>=3.1.0\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Collecting joblib>=1.2.0\n",
      "  Using cached joblib-1.5.3-py3-none-any.whl (309 kB)\n",
      "Collecting scipy>=1.8.0\n",
      "  Using cached scipy-1.15.3-cp310-cp310-win_amd64.whl (41.3 MB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\armin\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (26.0)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Using cached contourpy-1.3.2-cp310-cp310-win_amd64.whl (221 kB)\n",
      "Collecting kiwisolver>=1.3.1\n",
      "  Using cached kiwisolver-1.4.9-cp310-cp310-win_amd64.whl (73 kB)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Using cached fonttools-4.61.1-cp310-cp310-win_amd64.whl (1.6 MB)\n",
      "Collecting pillow>=8\n",
      "  Using cached pillow-12.1.1-cp310-cp310-win_amd64.whl (7.0 MB)\n",
      "Collecting pyparsing>=3\n",
      "  Using cached pyparsing-3.3.2-py3-none-any.whl (122 kB)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\armin\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Installing collected packages: pytz, tzdata, threadpoolctl, pyparsing, pillow, numpy, kiwisolver, joblib, fonttools, cycler, scipy, pandas, contourpy, scikit-surprise, scikit-learn, matplotlib, surprise, seaborn\n",
      "Successfully installed contourpy-1.3.2 cycler-0.12.1 fonttools-4.61.1 joblib-1.5.3 kiwisolver-1.4.9 matplotlib-3.10.8 numpy-2.2.6 pandas-2.3.3 pillow-12.1.1 pyparsing-3.3.2 pytz-2025.2 scikit-learn-1.7.2 scikit-surprise-1.1.4 scipy-1.15.3 seaborn-0.13.2 surprise-0.1 threadpoolctl-3.6.0 tzdata-2025.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fastparquet\n",
      "  Downloading fastparquet-2025.12.0-cp310-cp310-win_amd64.whl (669 kB)\n",
      "     -------------------------------------- 669.7/669.7 kB 5.3 MB/s eta 0:00:00\n",
      "Collecting pyarrow\n",
      "  Downloading pyarrow-23.0.0-cp310-cp310-win_amd64.whl (27.5 MB)\n",
      "     ---------------------------------------- 27.5/27.5 MB 5.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pandas>=1.5.0 in c:\\users\\armin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fastparquet) (2.3.3)\n",
      "Collecting cramjam>=2.3\n",
      "  Downloading cramjam-2.11.0-cp310-cp310-win_amd64.whl (1.7 MB)\n",
      "     ---------------------------------------- 1.7/1.7 MB 4.4 MB/s eta 0:00:00\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2026.2.0-py3-none-any.whl (202 kB)\n",
      "     -------------------------------------- 202.5/202.5 kB 4.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in c:\\users\\armin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fastparquet) (2.2.6)\n",
      "Requirement already satisfied: packaging in c:\\users\\armin\\appdata\\roaming\\python\\python310\\site-packages (from fastparquet) (26.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\armin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=1.5.0->fastparquet) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\armin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=1.5.0->fastparquet) (2025.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\armin\\appdata\\roaming\\python\\python310\\site-packages (from pandas>=1.5.0->fastparquet) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\armin\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.2->pandas>=1.5.0->fastparquet) (1.17.0)\n",
      "Installing collected packages: pyarrow, fsspec, cramjam, fastparquet\n",
      "Successfully installed cramjam-2.11.0 fastparquet-2025.12.0 fsspec-2026.2.0 pyarrow-23.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.26.4\n",
      "  Using cached numpy-1.26.4-cp310-cp310-win_amd64.whl (15.8 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.6\n",
      "    Uninstalling numpy-2.2.6:\n",
      "      Successfully uninstalled numpy-2.2.6\n",
      "Successfully installed numpy-1.26.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install surprise pandas scikit-learn matplotlib seaborn\n",
    "%pip install fastparquet pyarrow\n",
    "%pip install numpy==1.26.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f606503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f75ab37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_parquet('test_video_games.parquet')\n",
    "train_df = pd.read_parquet('train_video_games.parquet')\n",
    "items_highly_rated_freq = pd.read_csv('csv_files/item_highly_rated_freq.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82be2917",
   "metadata": {},
   "source": [
    "Based on the frequency of the most rated items computed in Week 6,\n",
    "implement the TopPop Recommender System, which always recommends\n",
    "the same top-k items sorted decreasingly by the number of “high” ratings\n",
    "(e.g., → 3) in the training split (train video games.parquet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "527090bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      item_id  highly_rated_count  average_rating_by_item\n",
      "0  B0086VPUHI                 154                4.425150\n",
      "1  B00BN5T30E                 150                4.428571\n",
      "2  B07YBXFDYN                 146                4.329268\n",
      "3  B00BGA9WK2                 136                4.432432\n",
      "4  B007CM0K86                 123                4.554688\n",
      "5  B00KIWEMIG                 109                4.237705\n",
      "6  B07YBWT3PK                 104                4.107438\n",
      "7  B07YBXFF99                 103                4.345455\n",
      "8  B014R4KYMS                 102                4.532110\n",
      "9  B004HD55VK                 102                4.615385\n"
     ]
    }
   ],
   "source": [
    "# implement based line TopPop recommender system\n",
    "top_pop_items = items_highly_rated_freq.head(10)\n",
    "# just recommend the top 10 most popular highly rated (defined as having a rating >= 3) items\n",
    "print(top_pop_items)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743e7ad1",
   "metadata": {},
   "source": [
    "Choose at least one neighborhood-based model and one latent factor model (both type of colaborative filtering)\n",
    "that uses the observed user-item ratings in the training set to predict the\n",
    "unobserved ratings. Report your choice of models.   \n",
    "\n",
    "Does unobserved mean empty cell? There is no missing data as seen in from the data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5520b6",
   "metadata": {},
   "source": [
    "Use 5-fold cross-validation on the training set to tune the hyperparameters\n",
    "of the chosen models (similarity measure and number of neighbors for the\n",
    "neighborhood-based model; number of latent factors and number of epochs\n",
    "for the latent factor model)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe56377",
   "metadata": {},
   "source": [
    "Choose an evaluation measure that is suitable for this task and justify your\n",
    "motivation in using it. Report the optimal hyperparameters together with\n",
    "the scores of your chosen measure, averaged over the 5 folds.\n",
    "• Run the models with the optimal hyperparameters to the whole training\n",
    "set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c98833d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert train and test to csv files\n",
    "# train_df.to_csv('train_video_games.csv', index=False)\n",
    "# test_df.to_csv('test_video_games.csv', index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32603b97",
   "metadata": {},
   "source": [
    "### Item Based KNN Colaborative Filtering\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "469140dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create item based KNN colaborative filtering recommender system using the surprise library\n",
    "# use 5-fold cross validation \n",
    "# use Root Mean Square Error (RMSE) as the evaluation metric\n",
    "from surprise import Dataset, Reader\n",
    "from surprise import KNNBasic, KNNBaseline\n",
    "from surprise.model_selection import cross_validate, GridSearchCV\n",
    "\n",
    "# load the data into surprise format\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(train_df[['user_id', 'item_id', 'rating']], reader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc83342a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "surprise.model_selection.search.GridSearchCV"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train/evaluate the algorithm using grind search 5 fold cross validation\n",
    "params = {'num_neighbors':[5, 10, 20], 'epochs':[5, 10, 15], 'sim_options': {'name': ['cosine', 'pearson'], 'user_based': [False]}}\n",
    "\n",
    "algo = KNNBaseline()\n",
    "cv_results = GridSearchCV(algo, params, cv=5)\n",
    "type(cv_results)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ebeac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cv_results.best_params)\n",
    "print(cv_results.best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93691ac8",
   "metadata": {},
   "source": [
    "The two similarity measure for nearest neighbor is Cosine Similarity, Pearson coefficent. Cosine Similarity measure is used for item based. Why?     \n",
    "We can also used euclidean distance or dot product if we wanted to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4deebb2a",
   "metadata": {},
   "source": [
    "### SVD \n",
    "SVD is a matrix factorization technique that decomposes the user-item interaction matrix into latent factors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8cd22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RMSE, MAE of algorithm SVD on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    0.9162  0.9267  0.9142  0.9136  0.9299  0.9201  0.0068  \n",
      "MAE (testset)     0.6575  0.6692  0.6611  0.6535  0.6643  0.6611  0.0054  \n",
      "Fit time          0.12    0.12    0.17    0.13    0.16    0.14    0.02    \n",
      "Test time         0.01    0.01    0.01    0.01    0.01    0.01    0.00    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_rmse': array([0.91619688, 0.92672458, 0.9142333 , 0.91355826, 0.92988788]),\n",
       " 'test_mae': array([0.65748713, 0.66920895, 0.66111258, 0.65353618, 0.6643198 ]),\n",
       " 'fit_time': (0.11883354187011719,\n",
       "  0.1232149600982666,\n",
       "  0.16502070426940918,\n",
       "  0.13321495056152344,\n",
       "  0.15529417991638184),\n",
       " 'test_time': (0.014001846313476562,\n",
       "  0.011770963668823242,\n",
       "  0.014071941375732422,\n",
       "  0.014963865280151367,\n",
       "  0.012979745864868164)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train a SVD based collabortative filtering recommender system using the surprise library\n",
    "from surprise import SVD\n",
    "\n",
    "algoSVD = SVD()\n",
    "cross_validate(algoSVD, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)\n",
    "# data and reader is the same \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160f8faa",
   "metadata": {},
   "source": [
    "MAE is mean absolute error performance metric for evaluating the accuracy of a recommender system. It measures the average absolute difference between the predicted ratings and the actual ratings. A lower MAE indicates better performance, as it means that the predictions are closer to the true ratings. MAE is commonly used in recommender systems to assess how well the model is predicting user preferences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36a5169",
   "metadata": {},
   "source": [
    "We also want to minimize Root Mean Squared Error (if its an error metric, minimize it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c80cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# which performance metric is better for evaluating recommender systems? RMSE or MAE?\n",
    "# RMSE is more sensitive to outliers than MAE, so it may be more appropriate for evaluating recommender systems that have a lot of outliers in the ratings. MAE is less sensitive to outliers, \n",
    "# So it may be more appropriate for evaluating recommender systems that have fewer outliers in the ratings. Ultimately, the choice of performance metric depends on the specific characteristics of the dataset and the goals of the evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fdf313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rmse': {'n_factors': 150, 'n_epochs': 40, 'lr_all': 0.01, 'reg_all': 0.1}, 'mae': {'n_factors': 150, 'n_epochs': 40, 'lr_all': 0.01, 'reg_all': 0.02}}\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters for SVD can be tuned using grid search\n",
    "# reg_all means regularization term for all parameters, n_factors is the number of latent factors\n",
    "params_svd = {'n_factors': [50, 100, 150], 'n_epochs': [20, 30, 40], 'lr_all': [0.002, 0.005, 0.01], 'reg_all': [0.02, 0.05, 0.1]}\n",
    "cv_results_svd = GridSearchCV(SVD, params_svd, cv=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2053c8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rmse': {'n_factors': 150, 'n_epochs': 40, 'lr_all': 0.01, 'reg_all': 0.1}, 'mae': {'n_factors': 150, 'n_epochs': 40, 'lr_all': 0.01, 'reg_all': 0.02}}\n",
      "{'rmse': 0.8852315270916729, 'mae': 0.5939371235752922}\n"
     ]
    }
   ],
   "source": [
    "# print the average RMSE\n",
    "print(cv_results_svd.best_params)\n",
    "print(cv_results_svd.best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18c20ed",
   "metadata": {},
   "source": [
    "I will use MAE as performance metrics. Root mean squared error gives more weight to outliers but we don't have a lot of outlier ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f58ff10",
   "metadata": {},
   "source": [
    "cv does what exactly?  \n",
    "cv creates 5 folds of the data, trains the model on 4 folds and tests on the last fold  \n",
    "each fold is run with the same hyperparameters, but different test fold   \n",
    "what does the cv tell us? it tells us the average performance of the   model with the given hyperparameters across the 5 folds,   \n",
    "which gives us an estimate of how well the model will perform on unseen data.  \n",
    "make sure folds are random and stratified  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd5283b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.matrix_factorization.SVD at 0x27043c63850>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the models with the optimal hyperparameters to the whole training set.\n",
    "best_svd = SVD(n_factors=150, n_epochs=40, lr_all=0.01, reg_all=0.02)\n",
    "best_svd.fit(data.build_full_trainset())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5afb6096",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\armin\\AppData\\Local\\Temp\\ipykernel_18324\\5977648.py:11: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  recommendations = pd.concat([recommendations, pd.DataFrame({'user_id': user_id, 'item_id': item_id, 'predicted_rating': predicted_rating}, index=[0])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            user_id     item_id  predicted_rating\n",
      "0      AE25ZDXYBK3LHKCZ7XUODANPME4A  B07BLRF329          2.970732\n",
      "1      AE25ZDXYBK3LHKCZ7XUODANPME4A  B0002A6CQ4          2.848447\n",
      "2      AE25ZDXYBK3LHKCZ7XUODANPME4A  B087NN2K41          2.806718\n",
      "3      AE25ZDXYBK3LHKCZ7XUODANPME4A  B01GY35UK6          2.796152\n",
      "4      AE25ZDXYBK3LHKCZ7XUODANPME4A  B00YQ74LVM          2.785425\n",
      "...                             ...         ...               ...\n",
      "13885  AHZYXDJ3HNLKS2E73VOSNIZZJT4Q  B07X5X5KF9          5.000000\n",
      "13886  AHZYXDJ3HNLKS2E73VOSNIZZJT4Q  B079C7QGLQ          5.000000\n",
      "13887  AHZYXDJ3HNLKS2E73VOSNIZZJT4Q  B00BGA9X9W          5.000000\n",
      "13888  AHZYXDJ3HNLKS2E73VOSNIZZJT4Q  B003O6E800          5.000000\n",
      "13889  AHZYXDJ3HNLKS2E73VOSNIZZJT4Q  B0083GAF9E          5.000000\n",
      "\n",
      "[13890 rows x 3 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\armin\\AppData\\Local\\Temp\\ipykernel_18324\\5977648.py:13: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top_recommendations = recommendations.groupby('user_id').apply(lambda x: x.nlargest(\n"
     ]
    }
   ],
   "source": [
    "# Use the final model to rank the non-rated items for each user\n",
    "# is meant by rank? \n",
    "user_ids = data.df['user_id'].unique()\n",
    "item_ids =data.df['item_id'].unique()\n",
    "# create a dataframe to store the recommendations for each user\n",
    "recommendations = pd.DataFrame(columns=['user_id', 'item_id', 'predicted_rating'])\n",
    "# O(num_users * num_items) runtime\n",
    "for user_id in user_ids:\n",
    "    for item_id in item_ids:\n",
    "        if not data.df[(data.df['user_id'] == user_id) & (data.df['item_id'] == item_id)].empty: \n",
    "            continue # if the user has already rated the item, skip it\n",
    "        predicted_rating = best_svd.predict(user_id, item_id).est # is the prediction the runtime long?\n",
    "        recommendations = pd.concat([recommendations, pd.DataFrame({'user_id': user_id, 'item_id': item_id, 'predicted_rating': predicted_rating}, index=[0])], ignore_index=True)\n",
    "# for each user, recommend the top 10 items with the highest predicted ratings\n",
    "top_recommendations = recommendations.groupby('user_id').apply(lambda x: x.nlargest(\n",
    "10, 'predicted_rating')).reset_index(drop=True)\n",
    "print(top_recommendations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
